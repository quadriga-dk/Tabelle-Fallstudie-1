
@misc{fair_data_austria_persistente_2021,
	title = {Persistente {Identifikatoren} ({PID})},
	copyright = {CC BY 4.0},
	url = {https://fair-office.at/pid/},
	author = {FAIR Data Austria},
	year = {2021},
}

@misc{berners-lee_universal_nodate,
	title = {Universal {Resource} {Identifiers}},
	url = {https://www.w3.org/Addressing/URL/uri-spec.html},
	author = {Berners-Lee, Tim},
}

@misc{noauthor_pid_2024,
	title = {{PID}},
	url = {https://forschungsdaten.info/themen/veroeffentlichen-und-archivieren/persistente-identifikatoren/},
	month = mar,
	year = {2024},
}

@misc{bruns_checkliste_2019,
	title = {Checkliste zur {Steigerung} der {Datenqualität} von {CSV}-{Dateien}},
	shorttitle = {{NQDM}-{Checkliste}-{CSV}},
	url = {https://nqdm-projekt.de/de/downloads/leitfaden},
	abstract = {Die Checkliste hilft dabei, die Qualität von CSV-Dateien zu überprüfen und auf einfache Weise mögliche Qualitätsprobleme zu identifizieren. Die Checkliste basiert auf dem NQDM-Leitfaden zur Steigerung der Qualität von Daten und Metadaten.
Dieser Leitfaden wurde im Rahmen des Projektes NQDM – Normentwurf für qualitativ hochwertige Daten und Metadaten – von Fraunhofer FOKUS im Zeitraum von September 2017 bis August 2019 erstellt. Weiterführende Informationen zu dem Projekt können unter https://www.nqdm-projekt.de/ eingesehen werden.},
	language = {Deutsch},
	publisher = {Fraunhofer-Institut für Offene Kommunikationssysteme FOKUS},
	author = {Bruns, Lina and Dittwald, Benjamin and Meiners, Fritz},
	year = {2019},
	keywords = {Datenqualität, Open Data},
}

@article{wilkinson_fair-prinzipien_2016,
	title = {Die {FAIR}-{Prinzipien} für das wissenschaftliche {Datenmanagement} und {Data} {Stewardship}},
	copyright = {Creative Commons Attribution 4.0 International, Open Access},
	url = {https://zenodo.org/record/6247015},
	doi = {10.5281/ZENODO.6247015},
	abstract = {Es besteht dringender Bedarf, die Infrastruktur für die Nachnutzung wissenschaftlicher Daten zu verbessern. Verschiedene Stakeholder aus Hochschulen, Industrieunternehmen, Forschungsförderern und wissenschaftlichen Verlagen haben sich zusammengeschlossen, um gemeinsam einen präzise gefassten Satz von messbaren Prinzipien zu formulieren und zu unterstützen. Diese Prinzipien, die wir als FAIR-Data-Prinzipien bezeichnen, sollen als Leitlinie für die Verbesserung der Nachnutzbarkeit von Datenbeständen dienen. Im Unterschied zu Peer-Initiativen, deren Fokus auf menschlichen Wissenschaftler*innen liegt, richten die FAIR-Prinzipien ein besonderes Augenmerk darauf, Maschinen das automatische Auffinden und die Verwendung von Daten zu erleichtern und somit eine Wiederverwendung durch Menschen zu unterstützen. Dieser Kommentar stellt die erste offizielle Veröffentlichung der FAIR-Prinzipen dar. Er beinhaltet die ihnen zu Grunde liegenden Überlegungen und beispielhaft einige exemplarische Implementierungen innerhalb der Community. {\textless}strong{\textgreater}Die vorliegende deutsche Übersetzung des Dokuments entstand im Rahmen des Verbundprojektes EcoDM mit der Förderung des Bundesministeriums für Bildung und Forschung (BMBF) unter dem Förderkennzeichen 16DWWQP.{\textless}/strong{\textgreater}},
	language = {de},
	urldate = {2024-07-26},
	author = {Wilkinson, Mark D. and Dumontier, Michel and Aalbersberg, IJsbrand Jan and Appleton, Gabrielle and Axton, Myles and Baak, Arie and Blomberg, Niklas and Boiten, Jan-Willem and da Silva Santos, Luiz Bonino and Bourne, Philip E. and Bouwman, Jildau and Brookes, Anthony J. and Clark, Tim and Crosas, Mercè and Dillo, Ingrid and Dumon, Olivier and Edmunds, Scott and Evelo, Chris T. and Finkers, Richard and Gonzalez-Beltran, Alejandra and Gray, Alasdair J.G. and Groth, Paul and Goble, Carole and Grethe, Jeffrey S. and Heringa, Jaap and 't Hoen, Peter A.C. and Hooft, Rob and Kuhn, Tobias and Kok, Ruben and Kok, Joost and Lusher, Scott J. and Martone, Maryann E. and Mons, Albert and Packer, Abel L. and Persson, Bengt and Rocca-Serra, Philippe and Roos, Marco and van Schaik, Rene and Sansone, Susanna-Assunta and Schultes, Erik and Sengstag, Thierry and Slater, Ted and Strawn, George and Swertz, Morris A. and Thompson, Mark and van der Lei, Johan and van Mulligen, Erik and Velterop, Jan and Waagmeester, Andra and Wittenburg, Peter and Wolstencroft, Katherine and Zhao, Jun and Mons, Barend},
	month = mar,
	year = {2016},
	keywords = {Datenmanagement, FAIR-Prinzipien, Forschungsdatenmanagement},
}

@article{deutsche_forschungsgemeinschaft_guidelines_2022,
	title = {Guidelines for {Safeguarding} {Good} {Research} {Practice}. {Code} of {Conduct}},
	copyright = {Creative Commons Attribution Share Alike 4.0 International, Open Access},
	url = {https://zenodo.org/record/6472827},
	doi = {10.5281/ZENODO.6472827},
	abstract = {The DFG´s Code of Conduct “Safeguarding Good Research Practice” represents the consensus among the member organisations of the DFG on the fundamental principles and standards of good practice and are upheld by these organisations. These guidelines underline the importance of integrity in the everyday practice of research and provide researchers with a reliable reference with which to embed good research practice as an established and binding aspect of their work.},
	language = {de},
	urldate = {2024-07-26},
	author = {{Deutsche Forschungsgemeinschaft}},
	month = apr,
	year = {2022},
	keywords = {DFG, Deutsche Forschungsgemeinschaft, German research foundation, Kodex, Leitlinien zur Sicherung guter wissenschaftlicher Praxis, Wissenschaftliche Integrität, code of conduct, good scientific practice, gute wissenschaftliche Praxis, research integrity, scientific misconduct, wissenschaftliches Fehlverhalten},
}

@misc{noauthor_open_2023,
	title = {Open {Data}, {Open} {Access} und {Nachnutzung}},
	url = {https://forschungsdaten.info/themen/finden-und-nachnutzen/open-data-open-access-und-nachnutzung/},
	month = may,
	year = {2023},
}

@misc{noauthor_what_2023,
	title = {What is the {Normalization} of databases?},
	shorttitle = {What is the {Normalization} of databases?},
	url = {https://databasecamp.de/en/data/normalization},
	abstract = {Learn about database normalization and how it can improve your database. Maximize efficiency and minimize redundancy with normalization.},
	language = {en-US},
	urldate = {2024-06-24},
	journal = {Data Basecamp},
	month = may,
	year = {2023},
	note = {Section: Data},
}

@article{codd_further_1971,
	title = {Further {Normalization} of the {Data} {Base} {Relational} {Model}},
	language = {en},
	author = {Codd, E F},
	month = aug,
	year = {1971},
}

@article{khodorovskii_normalization_2002,
	title = {On {Normalization} of {Relations} in {Relational} {Databases}},
	volume = {28},
	issn = {1608-3261},
	url = {https://doi.org/10.1023/A:1013759617481},
	doi = {10.1023/A:1013759617481},
	abstract = {A new normal form, namely, object-normal form (ONF), is introduced. It is shown that the existing definitions of the fifth normal form (5NF) are unsatisfactory. The correct definition is given for the first time. The importance of the 5NF is demonstrated. For improving data representation in a database with a 5NF schema, the notion of negating relation is introduced. It serves as a basis for the new normal form, namely, the sixth normal form. Combining requirements of the ONF and other normal forms, new normal forms are defined: the fourth object-normal form, the fifth object-normal form, and the sixth object-normal form. It is shown that the standard order of steps of the normalization procedure should be changed: first, the specific requirements of the fourth normal form should be satisfied, and only then the requirements of the second normal form, and so forth.},
	language = {en},
	number = {1},
	urldate = {2024-06-24},
	journal = {Programming and Computer Software},
	author = {Khodorovskii, V. V.},
	month = jan,
	year = {2002},
	pages = {41--52},
}

@article{broman_data_2018,
	title = {Data {Organization} in {Spreadsheets}},
	volume = {72},
	issn = {0003-1305},
	url = {https://doi.org/10.1080/00031305.2017.1375989},
	doi = {10.1080/00031305.2017.1375989},
	abstract = {Spreadsheets are widely used software tools for data entry, storage, analysis, and visualization. Focusing on the data entry and storage aspects, this article offers practical recommendations for organizing spreadsheet data to reduce errors and ease later analyses. The basic principles are: be consistent, write dates like YYYY-MM-DD, do not leave any cells empty, put just one thing in a cell, organize the data as a single rectangle (with subjects as rows and variables as columns, and with a single header row), create a data dictionary, do not include calculations in the raw data files, do not use font color or highlighting as data, choose good names for things, make backups, use data validation to avoid data entry errors, and save the data in plain text files.},
	number = {1},
	urldate = {2024-06-21},
	journal = {The American Statistician},
	author = {Broman, Karl W. and Woo, Kara H.},
	month = jan,
	year = {2018},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/00031305.2017.1375989},
	pages = {2--10},
}

@techreport{force11_joint_2014,
	title = {Joint {Declaration} of {Data} {Citation} {Principles}},
	url = {https://doi.org/10.25490/a97f-egyk},
	institution = {Data Citation Synthesis Group},
	author = {{FORCE11}},
	year = {2014},
}

@techreport{spiecker_ecodm_2022,
	title = {{EcoDM} - Ökosystem {Datenmanagement}: {Analysen} - {Empfehlungen} - {FAIRifizierung}},
	shorttitle = {{EcoDM} - Ökosystem {Datenmanagement}},
	url = {https://zenodo.org/records/6256398},
	abstract = {Mit dem BMBF geförderten Verbundprojekt EcoDM (Förderkennzeichen 16DWWQP) wurde erforscht, welche Herausforderungen, Chancen und Hindernisse sich im Bereich des rasanten digitalen Datenwachstums ergeben und wie Rahmenbedingungen aussehen könnten, Daten systematisch und FAIR (Findable, Accessible, Interoperable, Reusable) nutzen und teilen zu können. Einen zentralen Teil der Untersuchung bildeten neben Landscape- und Gap-Analysen Leitfaden-gestützte Interviews mit Expert*innen aus den Bereichen Wissenschaft, Wirtschaft, Public Sector und Qualifizierung. Basierend auf den Projektergebnissen wurden Empfehlungen zur Förderung des Teilens und Nachnutzens von Daten entwickelt. Der vorliegende Report veröffentlicht die gesammelten Untersuchungsergebnisse gemeinsam mit den 31 abgeleiteten bereichsübergreifenden Empfehlungen und die Diskussion dieser Empfehlungen im Rahmen der RDA Deutschland Tagung 2022.},
	language = {deu},
	urldate = {2024-06-06},
	institution = {Zenodo},
	author = {Spiecker, Claus and Richter, Janina and Walter, Paul and Chlastak, Maria and Burkart, Christine and Schneidenbach, Esther and Messerschmidt, Reinhard and Endres, Kirsten and Djeffal, Christian and Nürnberger, Eva},
	month = apr,
	year = {2022},
	doi = {10.5281/zenodo.6256398},
}

@book{noauthor_reproducibility_2019,
	title = {Reproducibility and {Replicability} in {Science}},
	isbn = {978-0-309-48619-4},
	abstract = {One of the pathways by which the scientific community confirms the validity of a new scientific discovery is by repeating the research that produced it. When a scientific effort fails to independently confirm the computations or results of a previous study, some fear that it may be a symptom of a lack of rigor in science, while others argue that such an observed inconsistency can be an important precursor to new discovery. Concerns about reproducibility and replicability have been expressed in both scientific and popular media. As these concerns came to light, Congress requested that the National Academies of Sciences, Engineering, and Medicine conduct a study to assess the extent of issues related to reproducibility and replicability and to offer recommendations for improving rigor and transparency in scientific research. Reproducibility and Replicability in Science defines reproducibility and replicability and examines the factors that may lead to non-reproducibility and non-replicability in research. Unlike the typical expectation of reproducibility between two computations, expectations about replicability are more nuanced, and in some cases a lack of replicability can aid the process of scientific discovery. This report provides recommendations to researchers, academic institutions, journals, and funders on steps they can take to improve reproducibility and replicability in science.},
	language = {en},
	publisher = {National Academies Press},
	author = {, Engineering, {and} Medicine, National Academies of Sciences},
	month = sep,
	year = {2019},
}

@article{parthenos_parthenos_2019,
	title = {{PARTHENOS} {Leitfaden} zur "{FAIRifizierung}" des {Datenmanagements} und der {Ermöglichung} der {Nachnutzung} von {Daten}},
	url = {https://zenodo.org/records/3363078},
	doi = {10.5281/zenodo.3363078},
	abstract = {A comprehensive set of  PARTHENOS Guidelines to FAIRify data management and make data reusable is focusing on the topic of common policies. This compact guide offers twenty guidelines to align the efforts of data producers, data archivists and data users in humanities and social sciences to make research data as reusable as possible based upon the FAIR Principles. Each guideline has recommendations for both researchers and archives as it is recognised that different priorities may apply to each case.


The guidelines result from the work of over fifty PARTHENOS project members. They were responsible for investigating commonalities in the implementation of policies and strategies for research data management and used results from desk research, questionnaires and interviews with selected experts to gather around one hundred current data management policies (including guides for preferred formats, data review policies and best practices, both formal as well as tacit).


Translation of "PARTHENOS Guidelines to FAIRify data management and make data reusable" (https://doi.org/10.5281/zenodo.2668479) into German by Laura Rothfritz, Claus Spiecker.


 


Further versions of the guidelines are available in the following languages:


Czech: "ZÁSADY zajištení FAIRové správy a využitelnosti dat" (https://doi.org/10.5281/zenodo.3946100)


English: "PARTHENOS Guidelines to FAIRify data management and make data reusable" (https://doi.org/10.5281/zenodo.3368858)


French: "PARTHENOS Recommandations pour FAIRiser vos données" (https://doi.org/10.5281/zenodo.3463521)


Greek: "PARTHENOS Οδηγίες για την εφαρμογή των αρχών FAIR στη διαχείριση και επανάχρηση δεδομένων" (https://doi.org/10.5281/zenodo.3363386)


Hungarian: "PARTHENOS A tudományos adatok újrafelhasználhatóságának és FAIR kezelésének irányelveii" (https://doi.org/10.5281/zenodo.3363355)


Italian: "PARTHENOS Linee guida per l’applicazione dei principi FAIR alla gestione e al riuso dei dati" (https://doi.org/10.5281/zenodo.3363243)


Portuguese: "Diretrizes para aplicação dos princípios FAIR à gestão e reutilização de dados" (https://doi.org/10.5281/zenodo.3937183)


Turkish: "Veri Yönetimi ve verinin yeniden kullanımı için FAIR Prensipleri Rehberi" (https://doi.org/10.5281/zenodo.3937149)},
	language = {deu},
	urldate = {2024-06-06},
	author = {PARTHENOS and Hollander, Hella and Morselli, Francesca and Uiterwaal, Frank and Admiraal, Femmy and Trippel, Thorsten and Di Giorgio, Sara},
	month = aug,
	year = {2019},
	note = {Publisher: Zenodo},
}

@incollection{heilsberger_empirische_2023,
	title = {Empirische {Verwaltungswissenschaft}},
	isbn = {978-3-658-39803-3},
	url = {https://doi.org/10.1007/978-3-658-39803-3},
	booktitle = {Empirische {Sozialforschung}
für die {Polizei}- und
Verwaltungswissenschaften},
	publisher = {Springer VS},
	author = {Heilsberger, Lars and Seyfried, Markus},
	year = {2023},
}

@misc{tib_anleitungen_nodate,
	title = {Anleitungen zur {DOI}-{Registrierung}},
	url = {https://projects.tib.eu/pid-service/tib-doi-konsortium/anleitungen-zur-doi-registrierung/},
	author = {{TIB}},
}

@misc{tib_digital_nodate,
	title = {Digital {Object} {Identifier} ({DOI})},
	url = {https://projects.tib.eu/pid-service/persistent-identifiers/digital-object-identifiers-dois/},
	author = {{TIB}},
}

@misc{tib_persistent_nodate,
	title = {Persistent {Identifiers} ({PIDs})},
	url = {https://projects.tib.eu/pid-service/persistent-identifiers/persistent-identifiers-pids/},
	author = {{TIB}},
}

@misc{noauthor_research_nodate,
	title = {Research {Organization} {Registry}},
	url = {https://ror.org/},
}

@misc{hollander_parthenos_nodate,
	title = {{PARTHENOS} {Leitfaden} zur "{FAIRifizierung}" des {Datenmanagements} und der {Ermöglichung} der {Nachnutzung} von {Daten}},
	url = {https://zenodo.org/records/3363078},
	author = {Hollander, Helli},
}

@misc{noauthor_orcid_nodate,
	title = {{ORCID}},
	url = {https://orcid.org/},
}

@article{jones_how_2017,
	title = {How {FAIR} are your data?},
	doi = {https://doi.org/10.5281/zenodo.5111307},
	author = {Jones, S. and Grootveld, M.},
	month = nov,
	year = {2017},
}

@misc{humboldt_universitat_zu_berlin_urn_2023,
	title = {{URN}},
	url = {https://www.ub.hu-berlin.de/de/bibliotheksglossar/urn},
	author = {{Humboldt Universität zu Berlin}},
	month = jan,
	year = {2023},
}

@misc{govdata_anleitung_2024,
	title = {Anleitung zur {Datenbereitstellung} auf {GovData}},
	url = {https://www.govdata.de/datenbereitstellungaufgovdata},
	author = {{GovData}},
	month = may,
	year = {2024},
}

@misc{go_fair_fair_nodate,
	title = {{FAIR} {Principles}},
	url = {https://www.go-fair.org/fair-principles/},
	author = {{GO FAIR}},
}

@misc{noauthor_gemeinsame_2022,
	title = {Gemeinsame {Normdatei} ({GND})},
	url = {https://www.dnb.de/DE/Professionell/Standardisierung/GND/gnd_node.html},
	month = aug,
	year = {2022},
}

@misc{buchner_persistente_2015,
	title = {Persistente {Identifikatoren} für unterschiedliche {Ressourcen} aller {Kultursparten}},
	copyright = {Deutsche Digitale Bibliothek},
	url = {https://www.deutsche-digitale-bibliothek.de/content/blog/persistente-identifikatoren-fuer-unterschiedliche-ressourcen-aller-kultursparten},
	author = {Büchner, Michael},
	month = apr,
	year = {2015},
}

@misc{bundesdruckerei_was_2023,
	title = {Was sind {FAIR} {Digital} {Objects}?},
	url = {https://www.bundesdruckerei.de/de/innovation-hub/was-sind-fair-digital-objects},
	author = {{Bundesdruckerei}},
	month = jan,
	year = {2023},
}

@article{syrian_virtual_university_damascus_syria_linked_2021,
	title = {Linked {Data}: {A} {Framework} for {Publishing} {FiveStar} {Open} {Government} {Data}},
	volume = {13},
	issn = {20749007, 20749015},
	shorttitle = {Linked {Data}},
	url = {https://www.mecs-press.org/ijitcs/ijitcs-v13-n6/v13n6-1.html},
	doi = {10.5815/ijitcs.2021.06.01},
	abstract = {With the increased adoption of open government initiatives around the world, a huge amount of governmental raw datasets was released. However, the data was published in heterogeneous formats and vocabularies and in many cases in bad quality due to inconsistency, messy, and maybe incorrectness as it has been collected by practicalities within the source organization, which makes it inefficient for reusing and integrating it for serving citizens and third-party apps.},
	language = {en},
	number = {6},
	urldate = {2024-06-01},
	journal = {International Journal of Information Technology and Computer Science},
	author = {{Syrian Virtual University, Damascus, Syria} and Al-khatib, Bassel and Ali, Ali Ahmad},
	month = dec,
	year = {2021},
	pages = {1--15},
}

@incollection{gangemi_assessing_2018,
	address = {Cham},
	title = {Assessing {FAIR} {Data} {Principles} {Against} the 5-{Star} {Open} {Data} {Principles}},
	volume = {11155},
	isbn = {978-3-319-98191-8 978-3-319-98192-5},
	url = {https://link.springer.com/10.1007/978-3-319-98192-5_60},
	abstract = {Access to biomedical data is increasingly important to enable data driven science in the research community. The Linked Open Data (LOD) principles (by Tim Berner-Lee) have been suggested to judge the quality of data by its accessibility (open data access), by its format and structures, and by its interoperability with other data sources. The objective is to use interoperable data sources across the Web with ease. The FAIR (ﬁndable, accessible, interoperable, reusable) data principles have been introduced for similar reasons with a stronger emphasis on achieving reusability. In this manuscript we assess the FAIR principles against the LOD principles to determine, to which degree, the FAIR principles reuse LOD principles, and to which degree they extend the LOD principles. This assessment helps to clarify the relationship between both schemes and gives a better understanding, what extension FAIR represents in comparison to LOD.},
	language = {en},
	urldate = {2024-06-01},
	booktitle = {The {Semantic} {Web}: {ESWC} 2018 {Satellite} {Events}},
	publisher = {Springer International Publishing},
	author = {Hasnain, Ali and Rebholz-Schuhmann, Dietrich},
	editor = {Gangemi, Aldo and Gentile, Anna Lisa and Nuzzolese, Andrea Giovanni and Rudolph, Sebastian and Maleshkova, Maria and Paulheim, Heiko and Pan, Jeff Z and Alam, Mehwish},
	year = {2018},
	doi = {10.1007/978-3-319-98192-5_60},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {469--477},
}

@book{groves_federal_2017,
	address = {Washington, D.C.},
	title = {Federal {Statistics}, {Multiple} {Data} {Sources}, and {Privacy} {Protection}: {Next} {Steps}},
	isbn = {978-0-309-46537-3},
	shorttitle = {Federal {Statistics}, {Multiple} {Data} {Sources}, and {Privacy} {Protection}},
	url = {https://www.nap.edu/catalog/24893},
	urldate = {2024-04-30},
	publisher = {National Academies Press},
	editor = {Groves, Robert M. and Harris-Kojetin, Brian A.},
	collaborator = {{Panel on Improving Federal Statistics for Policy and Social Science Research Using Multiple Data Sources and State-of-the-Art Estimation Methods} and {Committee on National Statistics} and {Division of Behavioral and Social Sciences and Education} and {National Academies of Sciences, Engineering, and Medicine}},
	month = dec,
	year = {2017},
	doi = {10.17226/24893},
	keywords = {Datenqualität, Qualitiy Framework},
}

@article{wickham_tidy_2014,
	title = {Tidy {Data}},
	volume = {59},
	copyright = {Copyright (c) 2013 Hadley  Wickham},
	issn = {1548-7660},
	url = {https://doi.org/10.18637/jss.v059.i10},
	doi = {10.18637/jss.v059.i10},
	abstract = {A huge amount of effort is spent cleaning data to get it ready for analysis, but there has been little research on how to make data cleaning as easy and effective as possible. This paper tackles a small, but important, component of data cleaning: data tidying. Tidy datasets are easy to manipulate, model and visualize, and have a specific structure: each variable is a column, each observation is a row, and each type of observational unit is a table. This framework makes it easy to tidy messy datasets because only a small set of tools are needed to deal with a wide range of un-tidy datasets. This structure also makes it easier to develop tidy tools for data analysis, tools that both input and output tidy datasets. The advantages of a consistent data structure and matching tools are demonstrated with a case study free from mundane data manipulation chores.},
	language = {en},
	urldate = {2024-04-26},
	journal = {Journal of Statistical Software},
	author = {Wickham, Hadley},
	month = sep,
	year = {2014},
	keywords = {Datenmanipulation},
	pages = {1--23},
}

@incollection{zehnder_datenmanipulation_1987,
	address = {Wiesbaden},
	title = {Datenmanipulation},
	isbn = {978-3-322-94122-0},
	url = {https://doi.org/10.1007/978-3-322-94122-0_4},
	abstract = {In einer Datenbank gespeicherte Daten haben für den Benutzer erst dann einen Sinn, wenn sie wieder abgefragt oder in anderer Weise benützt und bearbeitet, allgemein gesagt manipuliert werden können. Schon in der Übersicht (Unterabschnitt 1.5.3) haben wir dabei Abfragen und Mutationen unterschieden.— Abfragen bilden die häufigste Art, Daten zu manipulieren. Es stellt sich in diesem Zusammenhang das bei einer Datenbank nicht-triviale Problem der geeigneten Auswahl einer Teilmenge des Datenbestandes. In Kapitel 4 kommen die logischen Aspekte dieses Auswahlverfahrens zur Sprache, in Abschnitt 5.4 die physischen Zugriffspfade.— Auch bei Mutationen ist der Prozess der Datenauswahl von Bedeutung, da dem System mitgeteilt werden muss, welche Teilmenge der Daten verändert werden soil. Dazu kommen aber noch vielschichtige Probleme der Datenintegrität, denn bei jeder einzelnen Mutation muss man sicherstellen, dass der Inhalt der Datenbank konsistent bleibt. Ihrer Bedeutung wegen werden die Aspekte der Datenintegrität in Kapitel 6 zusammenfassend behandelt.},
	language = {de},
	urldate = {2024-04-26},
	booktitle = {Informationssysteme und {Datenbanken}},
	publisher = {Vieweg+Teubner Verlag},
	author = {Zehnder, Carl August},
	year = {1987},
	doi = {10.1007/978-3-322-94122-0_4},
	keywords = {Datenmanipulation},
	pages = {110--159},
}

@misc{noauthor_fundamental_2014,
	title = {Fundamental {Principles} of {National} {Official} {Statistics}},
	url = {https://unstats.un.org/unsd/dnss/gp/fundprinciples.aspx},
	publisher = {United Nations Statistics Divison},
	year = {2014},
	keywords = {Datenqualität, Statistikdaten},
}

@article{saidani_qualitatsdimensionen_2023,
	title = {Qualitätsdimensionen maschinellen {Lernens} in der amtlichen {Statistik}},
	volume = {17},
	issn = {1863-8163},
	url = {https://doi.org/10.1007/s11943-023-00329-7},
	doi = {10.1007/s11943-023-00329-7},
	abstract = {Die amtliche Statistik zeichnet sich durch ihren gesetzlich auferlegten Fokus auf die Qualität ihrer Veröffentlichungen aus. Dabei folgt sie den europäischen Qualitätsrahmenwerken, die auf nationaler Ebene in Form von Qualitätshandbüchern konkretisiert und operationalisiert werden, sich jedoch bis dato hinsichtlich Ausgestaltung und Interpretation an den Anforderungen der „klassischen“ Statistikproduktion orientieren. Der zunehmende Einsatz maschineller Lernverfahren (ML) in der amtlichen Statistik muss daher zur Erfüllung des Qualitätsanspruchs durch ein spezifisches, darauf zugeschnittenes Qualitätsrahmenwerk begleitet werden. Das vorliegende Papier leistet einen Beitrag zur Erarbeitung eines solchen Qualitätsrahmenwerks für den Einsatz von ML in der amtlichen Statistik, indem es (1) durch den Vergleich mit bestehenden Qualitätsgrundsätzen des Verhaltenskodex für Europäische Statistiken relevante Qualitätsdimensionen für ML identifiziert und (2) diese unter Berücksichtigung der besonderen methodischen Gegebenheiten von ML ausarbeitet. Dabei (2a) ergänzt es bestehende Vorschläge durch den Aspekt der Robustheit, (2b) stellt Bezug zu den Querschnittsthemen Machine Learning Operations (MLOps) und Fairness her und (2c) schlägt vor, wie die Qualitätssicherung der einzelnen Dimensionen in der Praxis der amtlichen Statistik ausgestaltet werden kann. Diese Arbeit liefert die konzeptionelle Grundlage, um Qualitätsindikatoren für ML-Verfahren formell in die Instrumente des Qualitätsmanagements im Statistischen Verbund zu überführen und damit langfristig den hohen Qualitätsstandard amtlicher Statistik auch bei Nutzung neuer Verfahren zu sichern.},
	language = {de},
	number = {3},
	urldate = {2024-04-24},
	journal = {AStA Wirtschafts- und Sozialstatistisches Archiv},
	author = {Saidani, Younes and Dumpert, Florian and Borgs, Christian and Brand, Alexander and Nickl, Andreas and Rittmann, Alexandra and Rohde, Johannes and Salwiczek, Christian and Storfinger, Nina and Straub, Selina},
	month = dec,
	year = {2023},
	pages = {253--303},
}

@misc{noauthor_verhaltenskodex_2018,
	title = {Verhaltenskodex für europäische {Statistiken}},
	url = {https://ec.europa.eu/eurostat/documents/4031688/9394019/KS-02-18-142-DE-N.pdf},
	publisher = {Amt für Veröffentlichungen der Europäischen Union},
	year = {2018},
	keywords = {Datenqualität, Statistikdaten},
}

@book{hader_empirische_2019,
	address = {Wiesbaden},
	title = {Empirische {Sozialforschung}: {Eine} {Einführung}},
	copyright = {http://www.springer.com/tdm},
	isbn = {978-3-658-26985-2 978-3-658-26986-9},
	shorttitle = {Empirische {Sozialforschung}},
	url = {http://link.springer.com/10.1007/978-3-658-26986-9},
	language = {de},
	urldate = {2024-04-24},
	publisher = {Springer Fachmedien Wiesbaden},
	author = {Häder, Michael},
	year = {2019},
	doi = {10.1007/978-3-658-26986-9},
	keywords = {Datenqualität, Empirische Sozialdaten},
}

@book{przyborski_qualitative_2014,
	address = {München},
	edition = {4., erweiterte Auflage},
	series = {Lehr- und {Handbücher} der {Soziologie}},
	title = {Qualitative {Sozialforschung}: {Ein} {Arbeitsbuch}},
	isbn = {978-3-486-70892-9},
	shorttitle = {Qualitative {Sozialforschung}},
	language = {de},
	publisher = {Oldenbourg Verlag},
	author = {Przyborski, Aglaja and Wohlrab-Sahr, Monika},
	year = {2014},
	keywords = {Datenqualität, Empirische Sozialdaten},
}

@article{neumaier_automated_2016,
	title = {Automated {Quality} {Assessment} of {Metadata} across {Open} {Data} {Portals}},
	volume = {8},
	issn = {1936-1955},
	url = {https://dl.acm.org/doi/10.1145/2964909},
	doi = {10.1145/2964909},
	abstract = {The Open Data movement has become a driver for publicly available data on the Web. More and more data—from governments and public institutions but also from the private sector—are made available online and are mainly published in so-called Open Data portals. However, with the increasing number of published resources, there is a number of concerns with regards to the quality of the data sources and the corresponding metadata, which compromise the searchability, discoverability, and usability of resources. In order to get a more complete picture of the severity of these issues, the present work aims at developing a generic metadata quality assessment framework for various Open Data portals: We treat data portals independently from the portal software frameworks by mapping the specific metadata of three widely used portal software frameworks (CKAN, Socrata, OpenDataSoft) to the standardized Data Catalog Vocabulary metadata schema. We subsequently define several quality metrics, which can be evaluated automatically and in an efficient manner. Finally, we report findings based on monitoring a set of over 260 Open Data portals with 1.1M datasets. This includes the discussion of general quality issues, for example, the retrievability of data, and the analysis of our specific quality metrics.},
	number = {1},
	urldate = {2024-04-23},
	journal = {Journal of Data and Information Quality},
	author = {Neumaier, Sebastian and Umbrich, Jürgen and Polleres, Axel},
	year = {2016},
	pages = {2:1--2:29},
}

@article{behkamal_metrics-driven_2014,
	title = {A metrics-driven approach for quality assessment of linked open data},
	volume = {9},
	issn = {0718-1876},
	doi = {10.4067/S0718-18762014000200006},
	abstract = {The main objective of the Web of Data paradigm is to crystallize knowledge through the interlinking of already existing but dispersed data. The usefulness of the developed knowledge depends strongly on the quality of the published data. Researchers have observed many deficiencies with regard to the quality of Linked Open Data. The first step towards improving the quality of data released as a part of the Linked Open Data Cloud is to develop tools for measuring the quality of such data. To this end, the main objective of this paper is to propose and validate a set of metrics for evaluating the inherent quality characteristics of a dataset before it is released to the Linked Open Data Cloud. These inherent characteristics are semantic accuracy, syntactic accuracy, uniqueness, completeness and consistency. We follow the Goal-Question-Metric approach to propose various metrics for each of these five quality characteristics. We provide both theoretical validation and empirical observation of the behavior of the proposed metrics in this paper. The proposed set of metrics establishes a starting point for a systematic inherent quality analysis of open datasets. © 2014 Universidad de Talca - Chile.},
	language = {English},
	number = {2},
	journal = {Journal of Theoretical and Applied Electronic Commerce Research},
	author = {Behkamal, B. and Kahani, M. and Bagheri, E. and Jeremic, Z.},
	year = {2014},
	keywords = {Datenqualität, Linked Data, Open Data},
	pages = {64--79},
}

@article{vetro_open_2016,
	title = {Open data quality measurement framework: {Definition} and application to {Open} {Government} {Data}},
	volume = {33},
	issn = {0740-624X},
	shorttitle = {Open data quality measurement framework},
	url = {https://www.sciencedirect.com/science/article/pii/S0740624X16300132},
	doi = {10.1016/j.giq.2016.02.001},
	abstract = {The diffusion of Open Government Data (OGD) in recent years kept a very fast pace. However, evidence from practitioners shows that disclosing data without proper quality control may jeopardize dataset reuse and negatively affect civic participation. Current approaches to the problem in literature lack a comprehensive theoretical framework. Moreover, most of the evaluations concentrate on open data platforms, rather than on datasets. In this work, we address these two limitations and set up a framework of indicators to measure the quality of Open Government Data on a series of data quality dimensions at most granular level of measurement. We validated the evaluation framework by applying it to compare two cases of Italian OGD datasets: an internationally recognized good example of OGD, with centralized disclosure and extensive data quality controls, and samples of OGD from decentralized data disclosure (municipality level), with no possibility of extensive quality controls as in the former case, hence with supposed lower quality. Starting from measurements based on the quality framework, we were able to verify the difference in quality: the measures showed a few common acquired good practices and weaknesses, and a set of discriminating factors that pertain to the type of datasets and the overall approach. On the basis of this evaluation, we also provided technical and policy guidelines to overcome the weaknesses observed in the decentralized release policy, addressing specific quality aspects.},
	number = {2},
	urldate = {2024-04-22},
	journal = {Government Information Quarterly},
	author = {Vetrò, Antonio and Canova, Lorenzo and Torchiano, Marco and Minotas, Camilo Orozco and Iemma, Raimondo and Morando, Federico},
	month = apr,
	year = {2016},
	keywords = {Datenqualität, Open Data},
	pages = {325--337},
}

@inproceedings{vaddepalli_taxonomy_2023,
	address = {Singapore},
	title = {Taxonomy of {Data} {Quality} {Metrics} in {Digital} {Citizen} {Science}},
	isbn = {978-981-19766-0-5},
	doi = {10.1007/978-981-19-7660-5_34},
	abstract = {Data quality is key in the success of a citizen science project. Valid datasets serve as evidence for scientific research. Numerous projects have highlighted the ways in which participatory data collection can cause data quality issues due to human day-to-day practices and biases. Also, these projects have used and reported a myriad of techniques to improve data quality in different contexts. Yet, there is a lack of systematic analyses of these experiences to guide the design and of digital citizen science projects. We mapped 35 data quality issues of 16 digital citizen science projects and proposed a taxonomy with 64 mechanisms to address data quality issues before, during and after the data collection in digital citizen science projects. This taxonomy is built upon the analysis of literature reports (N = 144), two urban experiments (participants = 280), and expert interviews (N = 11). Thus, we contribute to advance the development of systematic methods to improve the data quality in digital citizen science projects.},
	language = {en},
	booktitle = {Intelligent {Sustainable} {Systems}},
	publisher = {Springer Nature},
	author = {Vaddepalli, Krishna and Palacin, Victoria and Porras, Jari and Happonen, Ari},
	editor = {Nagar, Atulya K. and Singh Jat, Dharm and Mishra, Durgesh Kumar and Joshi, Amit},
	year = {2023},
	keywords = {Datenqualität, Open Data},
	pages = {391--410},
}

@article{pipino_data_2002,
	title = {Data {Quality} {Assessment}},
	volume = {45},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/505248.506010},
	doi = {10.1145/505248.506010},
	abstract = {How good is a company's data quality? Answering this question requires usable data quality metrics. Currently, most data quality measures are developed on an ad hoc basis to solve specific problems [6, 8], and fundamental principles necessary for developing usable metrics in practice are lacking. In this article, we describe principles that can help organizations develop usable data quality metrics.},
	language = {en},
	number = {4},
	urldate = {2024-04-22},
	journal = {Communications of the ACM},
	author = {Pipino, Leo L. and Lee, Yang W. and Wang, Richard Y.},
	month = apr,
	year = {2002},
	keywords = {Datenqualität, Open Data},
	pages = {211--218},
}

@book{bruns_leitfaden_2019,
	title = {Leitfaden für qualitativ hochwertige {Daten} und {Metadaten}},
	shorttitle = {{NQDM}-{Leitfaden}},
	url = {https://nqdm-projekt.de/de/downloads/leitfaden},
	abstract = {Dieser Leitfaden bietet praktische Hilfestellungen und Empfehlungen zur Erreichung einer hohen Datenund Metadatenqualität. Die enthaltenen Empfehlungen können grundsätzlich auf jegliche Art von Daten
angewendet werden, unabhängig von Zugänglichkeit, Herkunft und dem sektoralen Bezug. Besonders ist
der Leitfaden für Datenbereitsteller aus der öffentlichen Verwaltung empfehlenswert, die ihre Daten als
Open Data veröffentlichen.
Im Leitfaden werden unterschiedliche Qualitätsdimensionen, Datenstrukturtypen und Bewertungsschemata
für die Qualität von Daten und Metadaten aufgezeigt. Gängige maschinenlesbare und offene Daten- und
Schnittstellenformate werden vorgestellt und anhand anschaulicher Beispiele wird aufgezeigt, wie eine
hohe Datenqualität erreicht werden kann.
Der Leitfaden wurde im Rahmen des Projektes NQDM – Normentwurf für qualitativ hochwertige Daten und
Metadaten – von Fraunhofer FOKUS im Zeitraum von September 2017 bis August 2019 erstellt.
Weiterführende Informationen zu dem Projekt können unter https://www.nqdm-projekt.de/ eingesehen
werden.},
	language = {Deutsch},
	publisher = {Fraunhofer-Institut für Offene Kommunikationssysteme FOKUS},
	author = {Bruns, Lina and Dittwald, Benjamin and Meiners, Fritz},
	year = {2019},
	keywords = {Datenqualität, Open Data},
}

@misc{berners-lee_linked_2006,
	title = {Linked {Data}},
	url = {https://www.w3.org/DesignIssues/LinkedData.html},
	author = {Berners-Lee, Tim},
	month = jul,
	year = {2006},
}

@incollection{lindgren_linked_2019,
	address = {Cham},
	title = {Linked {Data} in the {European} {Data} {Portal}: {A} {Comprehensive} {Platform} for {Applying} {DCAT}-{AP}},
	volume = {11685},
	isbn = {978-3-030-27324-8 978-3-030-27325-5},
	shorttitle = {Linked {Data} in the {European} {Data} {Portal}},
	url = {https://link.springer.com/10.1007/978-3-030-27325-5_15},
	abstract = {Abstract
            
              The European Data Portal (EDP) is a central access point for metadata of Open Data published by public authorities in Europe and acquires data from more than 70 national data providers. The platform is a starting point in adopting the Linked Data specification DCAT-AP, aiming to increase interoperability and accessibility of Open Data. In this paper, we present the design of the central data management components of the platform, responsible for metadata storage, data harvesting and quality assessment. The core component is based on CKAN, which is extended by the support for native Linked Data replication to a triplestore to ensure legacy compatibility and the support for DCAT-AP. Regular data harvesting and the creation of detailed quality reports are performed by custom components adressing the requirements of DCAT-AP. The EDP is well on track to become the core platform for European Open Data and fostered the acceptance of DCAT-AP. Our platform is available here:
              https://www.europeandataportal.eu
              .},
	language = {en},
	urldate = {2024-04-09},
	booktitle = {Electronic {Government}},
	publisher = {Springer International Publishing},
	author = {Kirstein, Fabian and Dittwald, Benjamin and Dutkowski, Simon and Glikman, Yury and Schimmler, Sonja and Hauswirth, Manfred},
	editor = {Lindgren, Ida and Janssen, Marijn and Lee, Habin and Polini, Andrea and Rodríguez Bolívar, Manuel Pedro and Scholl, Hans Jochen and Tambouris, Efthimios},
	year = {2019},
	doi = {10.1007/978-3-030-27325-5_15},
	note = {Series Title: Lecture Notes in Computer Science},
	keywords = {Data quality, Linked Data},
	pages = {192--204},
}
